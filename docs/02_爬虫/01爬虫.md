# 爬虫

## 分类

通用爬虫：搜索引擎的爬虫
聚焦爬虫：针对特定网站的爬虫

- 模拟浏览器发送请求，获取响应

## 爬虫流程

- url->发送请求，获取相应->提取数据->保存
- 发送请求，获取相应->提取url

![](./../../images/01爬虫工作流程.png)

## 查看robots协议

Robots：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不可以抓取

- Robots.txt 是一个爬虫规范协议，看名称就知道它是一个 txt 的文本。放在网站的根目录下。
- robots.txt 文件由一条或多条规则组成。每条规则可禁止（或允许）特定抓取工具抓取相应网站中的指定文件路径。
- robots.txt的格式采用面向行的语法：空行、注释行（以#打头）、规则行。规则行的格式为：Field: value。

例如查看百度网盘的robots协议，输入此url：https://pan.baidu.com/robots.txt
淘宝的：https://www.taobao.com/robots.txt

### 表示含义

访问后查看到的部分如下：

```text
User-agent: Baiduspider
Disallow: /enterprise/
Disallow: /share/link/
Disallow: /wap/link/
Disallow: /share/home/
Disallow: /wap/share/home/
Disallow: /s/1*
Disallow: /api/shorturlinfo
Disallow: /share/list
Disallow: /share/wxlist
```

```text

User-Agent行:网页抓取工具的名称

User-Agent: robot-name
User-Agent: *

Disallow:不应抓取的目录或网页
Disallow: /path
Disallow:           # 空字符串，起通配符效果，全禁止

Allow:应抓取的目录或网页
Allow: /path
Allow:              # 空字符串，起通配符效果，全允许

Sitemap 网站的站点地图的位置
```

### 搜索引擎的User-Agent对应名称

|搜索引擎|User-Agent值|
|-----|-------|
|Google|googlebot|
|百度  |    baiduspider    |
|雅虎|slurp|
|MSN|msnbot|
|Alexa|is_archiver|
